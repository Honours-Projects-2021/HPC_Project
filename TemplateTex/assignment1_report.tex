


\title{COMS4040A \& COMS7045A Project  -- Report
    \\  K-Means and Fuzzy C-Means Clustering
    \\ Parallel Machine Learnig Algorithms
    }
\author{Shameel Nkosi, 1814731,  Coms Hons \\
        Siraj Motaung,1390537, BDA Hons}
\maketitle 
%\thispagestyle{empty}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\fancyhead[L]{COMS4040A \& COMS7045A Assinment 1}

%\vskip 3mm 
%\pagenumbering{roman}
\newpage
\pagenumbering{arabic} 

\begin{multicols*}{2}
    [
        \section{Introduction}
    ]
    

    In machine learning, clustering is a technique of grouping objects of similar characteristics into the same groups called clusters. The similarity between any two objects is measured by their distance from each other. \\ \\
    In this project, we have implemented the K-Means clustering algorithm and the Fuzzy C-Means algorithm. Traditionally, we aim to classify objects into separate clusters. The K-Means algorithm allows us to achieve this type of clustering, where an object belongs to only one cluster. On the other hand, Fuzzy C-Means allows us to classify an object as a member of different clusters.
    The K and C in K-Means and Fuzzy C-Means respectively are hyperparameters that specify the number of clusters or groups we wish to have. In this project we are going to implement these algoriths using three different implementations, namely, serial implementation in C++, parallel implementation in Cuda and in MPI. 

    \subsection{Problem Statement}
    In this project, we have taken a dataset from the \href{https://archive.ics.uci.edu/ml/datasets/wine}{ UCI Repository}. This data set describes the chemical composition of the wine. We want to use these chemical compositions to find the origins of the wine. These wines are from 3 different cultivars in the same region in Italy. We, therefore, aim to find out which of these wines belong to which cultivar.

    \section{Methodology}
    \subsection{Algorithms}
    This subsection describes the algorithms we implemented in-depth. 
    \subsubsection{K-Means Clustering}
    As mentioned above, K-Means partitions all objects into K clusters or subsets of the data set. Each object is an observation in our data set, which belongs to the nearest mean among the K means. Initially, we need to initialize the K mean as points in the same space as all the data points. The easiest way is to choose k random points as the initial means. Mathematically we can describe the problem as follows:
    Given a set of observations in a d-dimensional space,  K-Means partitions these observations into $k\leq n$ sets $S = \{S_1, S_2 , .. ,S_k\}$. We then want to find the minimum distance between each observation and the means. Formally, we to find:
    \begin{equation}
        \arg \min_S\sum_{i = 1}^{k}\sum_{\mathbf{x}\in S} \Vert \mathbf{x} - \mu_i \Vert^2   
    \end{equation}  
    K-Means is an iterative algorithm. We iteratively update the means until there is stability i.e. the means aren't changing anymore. In our implementation, however, we set several epochs so that we can measure the time it takes for each implementation. 
    The algorithm runs as follows:
    \begin{itemize}
        \item choose k random observations as your initial centroids or means
        \item Repeat for a specified number of epochs or until convergence:
        \begin{itemize}
            \item Calculate the distance of each data point with all the means.
            \item Assign each data point to a cluster with the least distance
            \item recalculate the means by assigning the mean to the average means of the data points that belong to that specific cluster
        \end{itemize}
    \end{itemize}

    Upon termination of the algorithm, each observation will belong to a specific cluster, in the case of our project, these observations will belong to one of three cluster.

    \subsubsection{Fuzzy-C Means}
    The difference between Fuzzy-C Means and K-Means is that K-Means partitions the data into different clusters, Fuzzy C-Means on the other hand assigns membership weight of each observation to a cluster. For example, let $\mathbf{x}$ be an observation, after running the K-Means algorithm, $\mathbf{x}$ can have the following classification: $[0 , 1 , 0]$, which means it belongs to second class or cluster. After running Fuzzy C-Means however, $\mathbf{x}$ can take an infinite possibilities, e.g $[0.12445 , 0.64587 , 0.24654]$ this means that the observation weighs more to the middle class then the left class then the right class.\\
    The algorithm works as follows:
    \begin{itemize}
        \item Choose a number of classes $\mathbf{C}$, in our case this will be 3.
        \item Randomly assign coefficients of observations being belonging to a cluster, let's call this the coefficients matrix $\mathbf{W}$.
        \item Repeat until convergence or for several iterations:
            \begin{itemize}
                \item compute the centroids or means for each cluster as follows
                \begin{equation}
                    \mathbf{c_k} = \frac{\sum_xw_k(x)^mx}{\sum_xw_k(x)^m}
                \end{equation}
                \item for each observation, computer it's coefficient of belonging to a cluster as follows:
                \begin{equation}
                    w_{ij} = \frac{1}{\sum_{k=1}^c(\frac{\Vert\mathbf{x}_i-\mathbf{c}_j \Vert}{\Vert\mathbf{x}_i-\mathbf{c}_k \Vert})^{\frac{1}{m-1}}}
                \end{equation}
            \end{itemize}
    \end{itemize}

    $m \geq 1$ in the above equation is the Fuzzy measure. $m$ is a hyperparameter, the bigger $m$ gets, the fuzzier the values. The smaller it is, the less fuzzy the coefficients will be.If $m$ is equal to one, then the algorithm becomes K-Means, this means that there isn't any fuzziness in the algorithm and clusters are disjoint. This means that there is a perfect partition of the dataset into clusters.

    \subsection{Solution implementations}
    The results of both algorithms are dependent on their initializations. The K-Means initializes centroids and Fuzzy C-Means initializes the coefficients matrix. For the K-Means, we initialized the clusters as the first 3 data points in the datasets, this helps to measure the correctness of all three implementations. As for the Fuzzy C-Means, we wrote a utility file that creates initializations for the coefficients and stores these coefficients into a csv file. All three implementations then read the dataset as well as the coefficients into their environment. The approaches above aid in validating the correctness of the implemented algorithms.

    \subsubsection{Serial Implementation}
    The serial was exactly as described in the the algorithms section above. 

    \subsubsection{MPI Parallel Implementation}
    To optimize for performance, we need to reduce the number of communications among processors. We can achieve this either by avoiding communications which can be difficult to do or by running the algorithm on a small number of processors.
    
    \textbf{K-Means}

    We began by dividing the data set almost equally across the data points. If the dataset can not be evenly split among the processors, then the last processors take the remainder of the data points. Each processor then calculates the number distance of its data points with all available clusters and assigns the data point to the nearest cluster. Upon distance calculations, each processor except the MASTER processors sends the distances as well as the assigning of clusters to the MASTER thread. The MASTER thread then broadcasts these results to every thread in the program. This is the first phase of the algorithm. 

    In the second phase, only c number of threads run, c is the number of clusters. Each running thread recalculates the position of the centroid assigned to it. The processors send these recalculated centroids to the MASTER thread and the MASTER thread broadcasts these centroids to every thread in the program. The above-mentioned process happens for several iterations. 
    
    \textbf{Fuzzy C-Means}

    The process here is the opposite of that in K-Means. We start with c number of threads among all available threads. These compute the centroids since we already have the initialized coefficient matrix. Each thread in the first phase computes the centroids and sends these centroids to the MASTER thread. The MASTER thread then broadcasts these to all threads in the program. 

    In the second phase, every thread in the program does approximately the same amount of work depending on whether the data points could be split evenly across the available threads. If the data can not be split evenly, the last thread takes the remainder of the data points.  Each thread then calculates the coefficients of each data point. Upon calculation, every thread sends its chunk to the MASTER thread and the MASTER thread broadcasts the results back to all the threads. At this point, the program is ready to move on to the next iteration.

    \subsubsection{CUDA Parallel Implementation}










\end{multicols*}


